\documentclass{article}
\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{amsmath}

\title{CS181 Assignment 5---Markov Decision Processes and Reinforcement Learning}
\author{Lucas Freitas and Angela Li}

\begin{document}
\maketitle
    
    \begin{enumerate}
        \item
            \begin{enumerate}[(a)]
                \item We have a probability distribution $P(\text{points } | \text{ target})$ and a utility function $U(\text{points, score})$. The expected utility of aiming for a target $t$ is:

                $$\sum_{k \in K} P(\text{k} | \text{t}) \; U(\text{k, S})$$

                Where $K$ is the set of possible points scored after one throw, and $S$ is the current score. To determine the optimal action, we use the maximum expected utility principle:

                $$\underset{t \in T}{\operatorname{argmax}} \sum_{k \in K} P(\text{k} | \text{t}) \; U(\text{k, S})$$

                Where T is the set of possible targets we can aim for.

                \item This utility function works fairly well for paring down the score in a relatively small number of dart throws*, but only until it reaches the point where it is possible to win in one throw. At that point, it makes decisions poorly in that it values a non-winning throw as nearly as good as a winning throw. \\

                For example, if the current score was 20, a throw resulting in a 20-point gain would end the game (and thus should be valued extremely highly). However, a throw resulting in a 19-point gain (which requires, at the very least, one more throw to win the game) would be valued at only 5\% less utility than a winning throw. So the proposed utility function is not conducive to good decision-making in states where it is possible to win in one move---in those cases, the winning move should be valued significantly more highly. \\

                *Even a quick paring-down of the score, however, is not always desirable. Consider that there are probably significantly many more ways to score 10 points in a throw than there are ways to score 1 point in a throw. However, if the current score was 20, the utility function would reward a score of 19 higher than a score of 10, even though both point values require at the very least one more dart throw to win; a throw of 10 shoul, however, be rewarded more because it creates more opportunity for a winning throw (of 10 points) than a throw of 19 (which requires a winning throw of 1 point).
            \end{enumerate}
        \item
            \begin{enumerate}[(a)]
                \item In our MDP model, the states are the possible scores in the game (so every integer in the range [0, \texttt{START\_SCORE}] is a state), and the actions are the possible areas (ring and wedge) that the dart player can aim for in any given turn. \\

                Therefore \texttt{get\_states()} should return \texttt{range(throw.START\_SCORE + 1)}.

                \item The reward function should not depend on the action $a$ - it should only take into account if the user has won ($s==0$) or not yet.  Thus, we can write the function as \texttt{return 1 if s == 0 else 0}. The problem about this reward function is that it doesn't show our preference of winning earlier than late.\\
                
                That is why the discount factor is important - we should use it to show that bias for earlier wins. If the discount factor was $1$, then we would still not be able to add that preference (we would still not care if we won early or late), and if the discount factor was $0$, that would mean that we don't care about winning later at all, which is not good, since we will definitely need future actions in order to win eventually. Thus, we should have a discount factor between $0$ and $1$, closer to $0$ in order to show the preference for winning early.

                \item % TODO: 2c
                
                \item There is no guarantee that the darts game can be won in a certain number of steps, so it doesn't make sense to impose an arbitrary finite horizon on the game---after all, the optimal policy should place primary importance on being able to get to a score of 0, and secondary importance on accomplishing that score in as few steps as possible.
                
                \item % TODO: 2e
                \item % TODO: 2f
            \end{enumerate}        
        \item
            \begin{enumerate}[(a)]
                \item We converge during policy iteration if $\pi_{i+1}(s) = \pi_{i}$ for every state $s$; that is, 

                $$\forall s \; V^{\pi_k}(s) = \underset{a}{\operatorname{argmax}} \sum_{s'} P(s' | s, a) [R(s, a) + \gamma V_j^{\pi_k}(s')]$$

                Since this satisfies the Bellman equation, we know that it is tha value of optimal stationary policy.
                \item If $\pi^{(1)}$ is the policy before a policy iteration phase and $\pi^{(1)}$ is the policy after, then by definition: $$\pi^{(2)}(s) = \underset{a}{\operatorname{argmax}} \; Q^{\pi^{(1)}}(s, a)$$

                
                \item Since there are a finite number of states and actions, there are also only a finite number of stationary policies. With every step of policy iteration, we will produce either an unchanged policy or a policy that is a strict improvement over the previous. If the policy is unchanged, then we terminate---producing an optimal policy as proven in part (a). If the policy is a strict improvement, we continue; however, since there are only a finite number of policies, we must eventually reach the optimal one as less-optimal policies are disregarded.
            \end{enumerate}        
        \item
            \begin{enumerate}[(a)]
                \item % TODO: 4a
                \item % TODO: 4b
                \item % TODO: 4c
            \end{enumerate}                
    \end{enumerate}

\end{document}